{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "find relevent part in docs\n",
    "load docs -> split into chunks -> embed -> vector store -> retriver (return chunks relevant to user question) -> search\n",
    "\n",
    "stuff: add chunks to prompt\n",
    "refine: update answers after anwering to every chunk\n",
    "map_reduce: find relevant parts in each chunk, combine, and create a final answer\n",
    "map_rerank: answer for each chunk while scoring, and create a final answer\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "if you want to search and use a whole file content\n",
    "load docs -> split into chunks\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load, split\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200, # max size of each chunk\n",
    "    chunk_overlap=50, # overlap between chunks\n",
    ")\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder( # .from_tiktoken_encoder(): count length of tokens (just how the model counts)\n",
    "    separator=\"\\n\", # split standard\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.txt\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "# embed\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "retriever = vectorstore.as_retriever() # input: user question, output: list of relevant doc chunks\n",
    "\n",
    "# search\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## off-the-shelf chain\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    ")\n",
    "chain.run(\"Describe Victory Mansions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Victory Mansions is a rundown residential building where the protagonist, Winston Smith, lives in the novel \"1984\" by George Orwell. It has glass doors that let in gritty dust, a hallway that smells of boiled cabbage and old rag mats, and a colored poster of an enormous face with the caption \"BIG BROTHER IS WATCHING YOU\" tacked to the wall. The flat in Victory Mansions is located seven flights up, with a faulty elevator and a telescreen on the wall that cannot be completely shut off. The building is part of a setting where an economy drive is in place in preparation for Hate Week. The roof of Victory Mansions offers a view of four similar pyramidal structures, one being the Ministry of Truth, indicating it is not as grand as the ministries. The flat itself has a unique layout with the telescreen in an unusual position and a shallow alcove where Winston can sit and remain outside the screen\\'s range.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## customized chain using lcel\n",
    "\n",
    "### stuff\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a helpful assistant. Answer questions using only the following context. \n",
    "            If you don't know the answer just say you don't know, don't make it up:\n",
    "            \\n\n",
    "            \\n\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm\n",
    "chain.invoke(\n",
    "    \"Describe Victory Mansions\"\n",
    ")  # \"Describe Victory Mansions\" is used in retriever and prompt\n",
    "\n",
    "### map_reduce\n",
    "find_relevant_parts_in_chunk_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Use the following portion of a long document to see if any of the text is relevant to answer the question.\\\n",
    "            Return any relevant text verbatim. If there is no relevant text, return : ''\n",
    "            -------\n",
    "            {chunk}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "find_relevant_parts_in_chunk_chain = find_relevant_parts_in_chunk_prompt | llm\n",
    "\n",
    "\n",
    "def combine_relevant_parts(inputs):\n",
    "    chunks = inputs[\"chunks\"]\n",
    "    question = inputs[\"question\"]\n",
    "    return \"\\n\\n\".join(\n",
    "        find_relevant_parts_in_chunk_chain.invoke(\n",
    "            {\"chunk\": chunk.page_content, \"question\": question}\n",
    "        ).content\n",
    "        for chunk in chunks\n",
    "    )\n",
    "\n",
    "\n",
    "combine_relevant_parts_chain = {\n",
    "    \"chunks\": retriever,  # list of chunks\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnableLambda(combine_relevant_parts)\n",
    "\n",
    "final_answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "            ------\n",
    "            {parts}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "final_answer_chain = (\n",
    "    {\"parts\": combine_relevant_parts_chain, \"question\": RunnablePassthrough()}\n",
    "    | final_answer_prompt\n",
    "    | llm\n",
    ")\n",
    "final_answer_chain.invoke(\"Describe Victory Mansions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
